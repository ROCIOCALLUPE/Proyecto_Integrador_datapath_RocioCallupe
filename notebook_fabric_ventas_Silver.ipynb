{"nbformat":4,"nbformat_minor":5,"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark","jupyter_kernel_name":null},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"dependencies":{"lakehouse":{"default_lakehouse":"0d1a30d0-e052-480d-80dc-4bd4faae2013","default_lakehouse_name":"LKH_PROY_INTEGRADOR_RCALL","default_lakehouse_workspace_id":"7afcf5b8-12f2-4f0b-a558-a0bd3b08a825","known_lakehouses":[{"id":"0d1a30d0-e052-480d-80dc-4bd4faae2013"}]}}},"cells":[{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# En este notebook aprenderemos a:\n","# Leer los datos con PySpark\n","# Realizar transformaciones\n","# Guardar los dataframes como tablas Delta\n","# Crear un modelo estrella para análisis\n","\n","#Configuración inicial y verificación del entorno\n","# Este comando verifica que estamos en el entorno correcto de Fabric\n","\n","import pyspark.sql.functions as F\n","from pyspark.sql.types import *\n","from notebookutils import mssparkutils\n","import pandas as pd\n","\n","# Verificar versión de Spark\n","print(f\"Versión de Spark: {spark.version}\")"],"outputs":[]},{"cell_type":"code","metadata":{"run_control":{"frozen":true},"editable":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# Crear estructura de carpetas en el lakehouse\n","# Este comando crea las carpetas necesarias en el lakehouse para almacenar nuestros archivos\n","\n","try:\n","    # Crear carpetas para datos brutos y procesados       \n","    mssparkutils.fs.mkdirs(\"Files/Bronze\")\n","    mssparkutils.fs.mkdirs(\"Files/silver\")\n","    mssparkutils.fs.mkdirs(\"Files/Gold\")\n","    print(\"✅ Estructura de carpetas creada correctamente\")\n","except Exception as e:\n","    print(f\"Error al crear carpetas: {str(e)}\")"],"outputs":[]},{"cell_type":"markdown","metadata":null,"source":["### Validación de datos"]},{"cell_type":"code","metadata":{"run_control":{"frozen":true},"editable":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["df_ = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Bronze/properties.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Bronze/brokers.csv\".\n","display(df_)"],"outputs":[]},{"cell_type":"code","metadata":{"run_control":{"frozen":true},"editable":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["df.printSchema()"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["\n","# Definición de esquemas para mejor control de los datos\n","# Definimos los esquemas de nuestros dataframes para asegurar tipos de datos correctos\n","\n","# Esquema para Brokers\n","schema_brokers = StructType([\n","    StructField(\"BrokerID\", IntegerType(), True),\n","    StructField(\"BrokerName\", StringType(), True),\n","    StructField(\"Region\", StringType(), True),\n","    StructField(\"Email\", StringType(), True)\n","])\n","\n","\n","\n","# Esquema para campaigns\n","schema_campaigns = StructType([\n","    StructField(\"CampaignID\", IntegerType(), True),\n","    StructField(\"Channel\", StringType(), True),\n","    StructField(\"CampaignName\", StringType(), True),\n","    StructField(\"StartDate\", DateType(), True),\n","    StructField(\"EndDate\", DateType(), True),\n","    StructField(\"BudgetUSD\", DoubleType(), True)\n","])\n","\n","# Esquema para clients\n","schema_clients = StructType([\n","    StructField(\"ClientID\", IntegerType(), True),\n","    StructField(\"FirstName\", StringType(), True),\n","    StructField(\"LastName\", StringType(), True),\n","    StructField(\"Email\", StringType(), True),\n","    StructField(\"Region\", StringType(), True)\n","])\n","\n","\n","# Esquema para projects\n","\n","schema_projects = StructType([\n","    StructField(\"ProjectID\", IntegerType(), True),\n","    StructField(\"ProjectName\", StringType(), True),\n","    StructField(\"City\", StringType(), True),\n","    StructField(\"Region\", StringType(), True),\n","    StructField(\"LaunchYear\", IntegerType(), True),\n","    StructField(\"Status\", StringType(), True)\n","])\n","\n","\n","# Esquema para properties\n","\n","schema_properties = StructType([\n","    StructField(\"PropertyID\", IntegerType(), True),\n","    StructField(\"ProjectID\", IntegerType(), True),\n","    StructField(\"PropertyType\", StringType(), True),\n","    StructField(\"Size_m2\", IntegerType(), True),\n","    StructField(\"Bedrooms\", IntegerType(), True),\n","    StructField(\"Bathrooms\", IntegerType(), True),\n","    StructField(\"ListPriceUSD\", DoubleType(), True),\n","    StructField(\"AvailabilityStatus\", StringType(), True)\n","])\n","\n","\n","# Esquema para lead\n","\n","schema_leads = StructType([\n","    StructField(\"LeadID\", IntegerType(), True),\n","    StructField(\"ClientID\", IntegerType(), True),\n","    StructField(\"PropertyID\", IntegerType(), True),\n","    StructField(\"CampaignID\", IntegerType(), True),\n","    StructField(\"LeadDate\", DateType(), True),\n","    StructField(\"LeadSource\", StringType(), True)\n","])\n","\n","\n","\n","\n","# Esquema para ventas\n","schema_sales = StructType([\n","    StructField(\"SaleID\", IntegerType(), True),\n","    StructField(\"PropertyID\", IntegerType(), True),\n","    StructField(\"ClientID\", IntegerType(), True),\n","    StructField(\"BrokerID\", IntegerType(), True),\n","    StructField(\"SaleDate\", DateType(), True),\n","    StructField(\"SalePriceUSD\", DoubleType(), True)\n","])\n","\n","print(\"✅ Esquemas definidos correctamente\")\n"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["# Leer los archivos CSV con los esquemas definidos\n","# Este comando lee los archivos CSV utilizando los esquemas definidos anteriormente\n","\n","try:\n","    # Leer archivo de Brokers\n","    df_brokers= spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema_brokers) \\\n","        .load(\"Files/Bronze/brokers.csv\")\n","    \n","    # Leer archivo de campaigns\n","    df_campaigns = spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema_campaigns) \\\n","        .load(\"Files/Bronze/campaigns.csv\")\n","    \n","    # Leer archivo de clients\n","    df_clients = spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema_clients) \\\n","        .load(\"Files/Bronze/clients.csv\")\n","    \n","    # Leer archivo de projects\n","    df_projects = spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema_projects) \\\n","        .load(\"Files/Bronze/projects.csv\")\n","\n","        # Leer archivo de properties\n","    df_properties = spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema_properties) \\\n","        .load(\"Files/Bronze/properties.csv\")    \n","\n","    # Leer archivo de ventas\n","    df_leads = spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema_leads) \\\n","        .load(\"Files/Bronze/leads.csv\")\n","\n","    # Leer archivo de ventas\n","    df_sales = spark.read.format(\"csv\") \\\n","        .option(\"header\", \"true\") \\\n","        .schema(schema_sales) \\\n","        .load(\"Files/Bronze/sales.csv\")\n","    \n","    print(\"✅ Archivos CSV leídos correctamente\")\n","except Exception as e:\n","    print(f\"Error al leer archivos: {str(e)}\")"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["\n","# Exploración de los datos\n","# Mostramos una vista previa de los datos cargados\n","\n","print(\"Vista previa de los datos de brokers:\")\n","display(df_brokers.limit(5))\n","\n","print(\"Vista previa de los datos de campaigns:\")\n","display(df_campaigns.filter(F.col(\"CampaignID\").isin('4009','4025')))\n","\n","print(\"Vista previa de los datos de projects:\")\n","display(df_projects.filter(F.col(\"ProjectID\")=='114'))\n","\n","print(\"Vista previa de los datos de clients:\")\n","display(df_clients.filter(F.col(\"ClientID\").isin('3020','3003')))\n","\n","print(\"Vista previa de los datos de properties:\")\n","display(df_properties.filter(F.col(\"PropertyID\")=='1007'))\n","\n","print(\"Vista previa de los datos de leads:\")\n","display(df_leads.filter(F.col(\"PropertyID\")=='1007'))\n","\n","print(\"Vista previa de los datos de sales:\")\n","display(df_sales.filter(F.col(\"PropertyID\")=='1007'))\n"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["print(\"Cantidad de registros por tabla:\\n\")\n","\n","print(f\"Brokers: {df_brokers.count()}\")\n","print(f\"Campaigns: {df_campaigns.count()}\")\n","print(f\"Projects: {df_projects.count()}\")\n","print(f\"Clients: {df_clients.count()}\")\n","print(f\"Properties: {df_properties.count()}\")\n","print(f\"Leads: {df_leads.count()}\")\n","print(f\"Sales: {df_sales.count()}\")"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["print(\"Cantidad de registros y esquema por tabla:\\n\")\n","\n","print(f\"Brokers: {df_brokers.count()}\")\n","df_brokers.printSchema()\n","\n","print(f\"\\nCampaigns: {df_campaigns.count()}\")\n","df_campaigns.printSchema()\n","\n","print(f\"\\nProjects: {df_projects.count()}\")\n","df_projects.printSchema()\n","\n","print(f\"\\nClients: {df_clients.count()}\")\n","df_clients.printSchema()\n","\n","print(f\"\\nProperties: {df_properties.count()}\")\n","df_properties.printSchema()\n","\n","print(f\"\\nLeads: {df_leads.count()}\")\n","df_leads.printSchema()\n","\n","print(f\"\\nSales: {df_sales.count()}\")\n","df_sales.printSchema()"],"outputs":[]},{"cell_type":"code","metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"source":["\n","# Guardar los dataframes como tablas Delta\n","# Guardamos los dataframes procesados como tablas Delta en el lakehouse\n","\n","try:\n","    # Guardar brokers\n","    df_brokers.write.format(\"delta\").mode(\"overwrite\").save(\"Files/Silver/brokers_delta\")\n","    \n","    # Guardar clients\n","    df_clients.write.format(\"delta\").mode(\"overwrite\").save(\"Files/Silver/clients_delta\")\n","   \n","\n","    # Guardar campaigns\n","    df_campaigns.write.format(\"delta\").mode(\"overwrite\").save(\"Files/Silver/campaigns_delta\")\n","    \n","    # Guardar projects\n","    df_projects.write.format(\"delta\").mode(\"overwrite\").save(\"Files/Silver/projects_delta\")\n","    \n","    # Guardar properties\n","    df_properties.write.format(\"delta\").mode(\"overwrite\").save(\"Files/Silver/properties_delta\")\n","    \n","    # Guardar leads\n","    df_leads.write.format(\"delta\").mode(\"overwrite\").save(\"Files/Silver/leads_delta\")\n","\n","    # Guardar Sales\n","    df_sales.write.format(\"delta\").mode(\"overwrite\").save(\"Files/Silver/Sales_delta\")\n","\n","    \n","    print(\"✅ Dataframes guardados como archivos Delta correctamente\")\n","except Exception as e:\n","    print(f\"Error al guardar archivos Delta: {str(e)}\")\n"],"outputs":[]}]}